# Create large (> 10GB) dataset for Gradient March Feature Team testing
# This is prior to release of New Notebook Datasets, Jupyter IDE, and Online/Offline viewing
# Also so can add section in docs based upon working example
#
# Download data from source and populate Gradient Dataset visible to March Feature Team
# Use NYC Taxi data since easily available and exceeds 10GB
# Requires the output dataset to have been pre-created
#
# Based on https://github.com/Paperspace/gradient-public-datasets/blob/main/mpd-coco.yaml
# but in general just Workflows in that repo
# Updated to use wget instead of curl
#
# Last updated: Mar 23rd 2022

defaults:
  resources:
    instance-type: C5 # Doesn't need GPU; C4s are showing as not available as of Mar 23

jobs:

  # 1. Download data and populate the public dataset
  # Output is 6 months of Taxi data, which exceeds 10GB
  # Catenate the 6 files into 1
  # Runtime is ...

  downloadData:
    outputs:
      nyc_taxi_large:
        type: dataset
        with:
          ref: March\ Feature\ Team/nyc_taxi_large
    uses: script@v1
    with:
      script: |-
        #apt-get update
        #apt-get install wget -y
        echo "Downloading Jan 2016 ..."
        wget -c 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2016-01.csv'
        echo "Downloading Feb 2016 ..."
        wget -c 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2016-02.csv'
        echo "Downloading Mar 2016 ..."
        wget -c 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2016-03.csv'
        echo "Downloading Apr 2016 ..."
        wget -c 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2016-04.csv'
        echo "Downloading May 2016 ..."
        wget -c 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2016-05.csv'
        echo "Downloading Jun 2016 ..."
        wget -c 'https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2016-06.csv'
        cd /outputs/nyc_taxi_large
        echo "MD5 sums of downloaded data:"
        md5sum yellow_tripdata_2016-01.csv
        md5sum yellow_tripdata_2016-02.csv
        md5sum yellow_tripdata_2016-03.csv
        md5sum yellow_tripdata_2016-04.csv
        md5sum yellow_tripdata_2016-05.csv
        md5sum yellow_tripdata_2016-06.csv
        echo "List of output files ..."
        echo "/outputs/nyc_taxi_large/:"
        ls "-aFl" /outputs/nyc_taxi_large
        echo "Done"
      image: ubuntu:20.04

  # 2. Check dataset can be viewed

  viewData:
    needs:
      - downloadData
    inputs:
      nyc_taxi_large: downloadData.outputs.nyc_taxi_large
    uses: script@v1
    with:
      script: |-
        echo "List of input files ..."
        echo "/inputs/:"
        ls "-aFl" /inputs
        echo "/inputs/nyc_taxi_large/:"
        ls "-aFl" /inputs/nyc_taxi_large
        echo "Done"
      image: ubuntu:20.04

# Appendix: Details
#
# The bash:5 Alpine image does not have apt and presumably not wget so use ubuntu:20.04 and install wget
# ubuntu:20.04 is from the standard Docker hub images at https://hub.docker.com/search?q=&type=image&image_filter=official
# There is 22:04 but not used much on Paperspace yet
# apt-get is better in scripts than apt, whose usage gives a warning
# ls -aFl shows any hidden files, appends "/" to directories, and shows file sizes & permissions
# This Workflow is being run on the Gradient container paperspace/nb-tensorflow:22.02-tf2-py3 but TensorFlow is not needed